{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Training will use CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorch_environment\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\pytorch_environment\\pytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Libraries imported and NLTK tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the NLTK tokenizer is available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "print(\"Libraries imported and NLTK tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples, tokenizer, max_input_length=1024, max_target_length=128):\n",
    "    \"\"\"Preprocess the data for training.\"\"\"\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        [doc for doc in examples[\"highlights\"]],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE metrics.\"\"\"\n",
    "    rouge_score = evaluate.load(\"rouge\")\n",
    "    predictions, labels = eval_pred\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    return {key: round(value * 100, 2) for key, value in result.items()}\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cnn_dailymail\"\n",
    "dataset_config = \"3.0.0\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(dataset_name, dataset_config)\n",
    "print(\"Dataset loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n",
      "Model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(\"Model and tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting subsets for training and validation...\n",
      "Training dataset size: 1000\n",
      "Validation dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "# Limit the dataset size for training and validation\n",
    "train_size = 1000\n",
    "val_size = 100\n",
    "\n",
    "print(\"Selecting subsets for training and validation...\")\n",
    "small_train_dataset = dataset[\"train\"].select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].select(range(val_size))\n",
    "\n",
    "print(f\"Training dataset size: {len(small_train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(small_val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the sliced dataset...\n",
      "Subset preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing the sliced dataset...\")\n",
    "tokenized_train_dataset = small_train_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer, max_input_length, max_target_length),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Preprocessing training dataset\",\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = small_val_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer, max_input_length, max_target_length),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    "    desc=\"Preprocessing validation dataset\",\n",
    ")\n",
    "\n",
    "print(\"Subset preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorch_environment\\pytorch\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./fine_tuned_summarizer\"\n",
    "num_train_epochs = 3\n",
    "batch_size = 2\n",
    "learning_rate = 5e-5\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    fp16=True if torch.cuda.is_available() else False,  \n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_pin_memory=True,  \n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Training arguments set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset= tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/375 [00:02<15:23,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2744, 'grad_norm': inf, 'learning_rate': 5e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 100/375 [18:18<49:18, 10.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0975, 'grad_norm': 2.664755344390869, 'learning_rate': 3.68e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 125/375 [25:21<45:41, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5987657308578491, 'eval_rouge1': 35.72, 'eval_rouge2': 15.26, 'eval_rougeL': 25.35, 'eval_rougeLsum': 32.85, 'eval_runtime': 145.1947, 'eval_samples_per_second': 0.689, 'eval_steps_per_second': 0.344, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 200/375 [39:05<31:23, 10.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4182, 'grad_norm': 2.5535361766815186, 'learning_rate': 2.3466666666666667e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 250/375 [50:38<22:52, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6577863097190857, 'eval_rouge1': 35.2, 'eval_rouge2': 14.9, 'eval_rougeL': 25.2, 'eval_rougeLsum': 32.56, 'eval_runtime': 143.3548, 'eval_samples_per_second': 0.698, 'eval_steps_per_second': 0.349, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 300/375 [59:47<13:27, 10.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2677, 'grad_norm': 2.0699822902679443, 'learning_rate': 1.0133333333333333e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [1:13:26<00:00, 10.71s/it]d:\\pytorch_environment\\pytorch\\Lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [1:15:49<00:00, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7513128519058228, 'eval_rouge1': 36.18, 'eval_rouge2': 14.76, 'eval_rougeL': 24.67, 'eval_rougeLsum': 33.23, 'eval_runtime': 136.0607, 'eval_samples_per_second': 0.735, 'eval_steps_per_second': 0.367, 'epoch': 3.0}\n",
      "{'train_runtime': 4549.8992, 'train_samples_per_second': 0.659, 'train_steps_per_second': 0.082, 'train_loss': 0.5324372838338216, 'epoch': 3.0}\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  9 08:57:52 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| 44%   59C    P3             63W /  200W |    7994MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3988    C+G   ...on\\131.0.2903.86\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      4244    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A      4348    C+G   ...804_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A      5588    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A      6164    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A      6692    C+G   ...804_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A      6816    C+G   ....0_x64__8wekyb3d8bbwe\\XboxPcApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10120    C+G   ...ejd91yc\\AdobeNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     14096    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14844    C+G   D:\\vscode\\Microsoft VS Code\\Code.exe        N/A      |\n",
      "|    0   N/A  N/A     15588    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     15860    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     17836    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     23040    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A     24264    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     25424    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     25728    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     25964    C+G   ...al\\Discord\\app-1.0.9173\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A     27692    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     30872      C   ...orch_environment\\pytorch\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     31728    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     31948    C+G   ...on\\HEX\\Creative Cloud UI Helper.exe      N/A      |\n",
      "|    0   N/A  N/A     33824    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     34824    C+G   ...on\\131.0.2903.86\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     35268    C+G   ...5.8.2.0_x64__htrsf667h5kn2\\AWCC.exe      N/A      |\n",
      "|    0   N/A  N/A     35620    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     36264    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     36452    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     36848    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     37028    C+G   ...s\\System32\\ApplicationFrameHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"empty\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
