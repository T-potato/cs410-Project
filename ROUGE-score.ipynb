{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorch_environment\\pytorch\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load pre-trained tokenizers and models\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "bart_original_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart_original_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "\n",
    "t5_large_tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "t5_large_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_summarizer/checkpoint-375\")\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"./fine_tuned_summarizer/checkpoint-375\")\n",
    "\n",
    "def preprocess_article(article: str, model_type: str = \"bart\", max_length: int = 1024):\n",
    "    \"\"\"\n",
    "    Preprocess the input article by tokenizing it using the specified model type.\n",
    "    \n",
    "    Args:\n",
    "        article (str): The input article text to preprocess.\n",
    "        max_length (int): Maximum token length for compatibility with the selected model.\n",
    "        model_type (str): The model type to use for tokenization. Options are \"bart\", \"t5\", \"bart-original\", \"t5-large\", \"fine-tuned\".\n",
    "    \n",
    "    Returns:\n",
    "        dict: Tokenized input ready for summarization.\n",
    "    \"\"\"\n",
    "    if model_type == \"t5\":\n",
    "        tokenized_input = t5_tokenizer(\n",
    "            article,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    elif model_type == \"t5-large\":\n",
    "        tokenized_input = t5_large_tokenizer(\n",
    "            article,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    elif model_type == \"bart-original\":\n",
    "        tokenized_input = bart_original_tokenizer(\n",
    "            article,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    elif model_type == \"bart\":\n",
    "        tokenized_input = bart_tokenizer(\n",
    "            article,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    elif model_type == \"fine-tuned\":\n",
    "        tokenized_input = fine_tuned_tokenizer(\n",
    "            article,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article: str, model_type: str = \"bart\", max_input_length: int = 1024, max_summary_length: int = 150):\n",
    "    \"\"\"\n",
    "    Generate a summary of the input article using either T5 or BART.\n",
    "    \n",
    "    Args:\n",
    "        article (str): The input article text.\n",
    "        model_type (str): Type of model, either 't5' or 'bart'.\n",
    "        max_input_length (int): Max length of the tokenized input.\n",
    "        max_summary_length (int): Max length of the generated summary.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated summary.\n",
    "    \"\"\"\n",
    "    # Select the appropriate model and tokenizer\n",
    "    if model_type == \"t5\":\n",
    "        model = t5_model\n",
    "        tokenizer = t5_tokenizer\n",
    "    elif model_type == \"bart\":\n",
    "        model = bart_model\n",
    "        tokenizer = bart_tokenizer\n",
    "    elif model_type == \"t5-large\":\n",
    "        model = t5_large_model\n",
    "        tokenizer = t5_large_tokenizer\n",
    "    elif model_type == \"bart-original\":\n",
    "        model = bart_original_model\n",
    "        tokenizer = bart_original_tokenizer\n",
    "    elif model_type == \"fine-tuned\":\n",
    "        model = fine_tuned_model\n",
    "        tokenizer = fine_tuned_tokenizer\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose 't5' or 'bart'.\")\n",
    "\n",
    "    # Preprocess the article\n",
    "    tokenized_input = preprocess_article(article, model_type=model_type, max_length=max_input_length)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        tokenized_input['input_ids'],\n",
    "        max_length=max_summary_length,\n",
    "        min_length=30,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "\n",
    "def calculate_rouge(dataset, model_type=\"bart\", max_input_length=1024, max_summary_length=150):\n",
    "    \"\"\"\n",
    "    Evaluates ROUGE scores for the last 100 examples in the dataset with progress display.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset with 'article' and 'highlights' columns.\n",
    "        model_type (str): Type of model, such as 't5', 'bart', or 'fine-tuned'.\n",
    "        max_input_length (int): Maximum token length for input.\n",
    "        max_summary_length (int): Maximum token length for generated summary.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Average ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    # Target the last 100 examples in the dataset\n",
    "    start_idx = max(0, len(dataset) - 5)\n",
    "\n",
    "    # Progress bar\n",
    "    print(\"Calculating ROUGE scores...\")\n",
    "    for idx in tqdm(range(start_idx, len(dataset)), desc=\"Processing examples\", unit=\"example\"):\n",
    "        example = dataset[idx]\n",
    "\n",
    "        # Generate the summary\n",
    "        generated_summary = generate_summary(\n",
    "            example[\"article\"],\n",
    "            model_type=model_type,\n",
    "            max_input_length=max_input_length,\n",
    "            max_summary_length=max_summary_length\n",
    "        )\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(example[\"highlights\"], generated_summary)\n",
    "        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge1 = np.mean(rouge1_scores)\n",
    "    avg_rouge2 = np.mean(rouge2_scores)\n",
    "    avg_rougeL = np.mean(rougeL_scores)\n",
    "\n",
    "    return {\n",
    "        \"Average ROUGE-1\": avg_rouge1,\n",
    "        \"Average ROUGE-2\": avg_rouge2,\n",
    "        \"Average ROUGE-L\": avg_rougeL\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 5/5 [01:56<00:00, 23.22s/example]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART ROUGE Scores: {'Average ROUGE-1': 0.4361984024780211, 'Average ROUGE-2': 0.21323528703624234, 'Average ROUGE-L': 0.27493072514747563}\n",
      "Calculating ROUGE scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 5/5 [01:52<00:00, 22.56s/example]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 ROUGE Scores: {'Average ROUGE-1': 0.4074326829103899, 'Average ROUGE-2': 0.18751513578838058, 'Average ROUGE-L': 0.2605476358979543}\n",
      "Calculating ROUGE scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 5/5 [00:58<00:00, 11.72s/example]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART ROUGE Scores: {'Average ROUGE-1': 0.3800909693730864, 'Average ROUGE-2': 0.14299618242863085, 'Average ROUGE-L': 0.20040745888424674}\n",
      "Calculating ROUGE scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 5/5 [04:47<00:00, 57.51s/example]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5-Large ROUGE Scores: {'Average ROUGE-1': 0.4470120918673608, 'Average ROUGE-2': 0.19643275829011073, 'Average ROUGE-L': 0.23771303828896687}\n",
      "Calculating ROUGE scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 5/5 [01:30<00:00, 18.11s/example]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned ROUGE Scores: {'Average ROUGE-1': 0.3978393468182637, 'Average ROUGE-2': 0.17779812193129052, 'Average ROUGE-L': 0.1903461327233912}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "\n",
    "# Evaluate with BART\n",
    "bart_rouge_scores = calculate_rouge(dataset, model_type=\"bart\")\n",
    "print(\"BART ROUGE Scores:\", bart_rouge_scores)\n",
    "\n",
    "# Evaluate with T5\n",
    "t5_rouge_scores = calculate_rouge(dataset, model_type=\"t5\")\n",
    "print(\"T5 ROUGE Scores:\", t5_rouge_scores)\n",
    "\n",
    "# Evaluate with BART\n",
    "bart_base_rouge_scores = calculate_rouge(dataset, model_type=\"bart-original\")\n",
    "print(\"BART ROUGE Scores:\", bart_base_rouge_scores)\n",
    "\n",
    "# Evaluate with T5\n",
    "t5_large_rouge_scores = calculate_rouge(dataset, model_type=\"t5-large\")\n",
    "print(\"T5-Large ROUGE Scores:\", t5_large_rouge_scores)\n",
    "\n",
    "# Evaluate with fine-tuned model\n",
    "fine_tuned_rouge_scores = calculate_rouge(dataset, model_type=\"fine-tuned\")\n",
    "print(\"Fine-tuned ROUGE Scores:\", fine_tuned_rouge_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
